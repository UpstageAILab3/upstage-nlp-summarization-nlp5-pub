{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EKOBTKH47Qy2"
      },
      "outputs": [],
      "source": [
        "# **ğŸ’ğŸ»ğŸ—¨ï¸ğŸ’ğŸ»â€â™‚ï¸ëŒ€í™” ìš”ì•½ Baseline code**\n",
        "> **Dialogue Summarization** ê²½ì§„ëŒ€íšŒì— ì˜¤ì‹  ì—¬ëŸ¬ë¶„ í™˜ì˜í•©ë‹ˆë‹¤! ğŸ‰\n",
        "> ë³¸ ëŒ€íšŒì—ì„œëŠ” ìµœì†Œ 2ëª…ì—ì„œ ìµœëŒ€ 7ëª…ì´ ë“±ì¥í•˜ì—¬ ë‚˜ëˆ„ëŠ” ëŒ€í™”ë¥¼ ìš”ì•½í•˜ëŠ” BART ê¸°ë°˜ ëª¨ë¸ì˜ baseline codeë¥¼ ì œê³µí•©ë‹ˆë‹¤.\n",
        "> ì£¼ì–´ì§„ ë°ì´í„°ë¥¼ í™œìš©í•˜ì—¬ ì¼ìƒ ëŒ€í™”ì— ëŒ€í•œ ìš”ì•½ì„ íš¨ê³¼ì ìœ¼ë¡œ ìƒì„±í•˜ëŠ” ëª¨ë¸ì„ ë§Œë“¤ì–´ë´…ì‹œë‹¤!\n",
        "## âš™ï¸ ë°ì´í„° ë° í™˜ê²½ì„¤ì •\n",
        "### 1) í•„ìš”í•œ ë¼ì´ë¸ŒëŸ¬ë¦¬ ì„¤ì¹˜\n",
        "- í•„ìš”í•œ ë¼ì´ë¸ŒëŸ¬ë¦¬ë¥¼ ì„¤ì¹˜í•œ í›„ ë¶ˆëŸ¬ì˜µë‹ˆë‹¤.\n",
        "\n",
        "!pip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu118\n",
        "\n",
        "\n",
        "# í•„ìš”í•œ ë¼ì´ë¸ŒëŸ¬ë¦¬ ì„¤ì¹˜\n",
        "!pip install wandb\n",
        "!pip install matplotlib\n",
        "!pip install pandas\n",
        "!pip install lightning\n",
        "!pip install pytorch-lightning\n",
        "!pip install --upgrade lightning\n",
        "!pip install rouge\n",
        "!pip install transformers\n",
        "!pip install transformers[torch]\n",
        "!pip install accelerate -U\n",
        "\n",
        "\n",
        "import pandas as pd\n",
        "import os\n",
        "import re\n",
        "import json\n",
        "import yaml\n",
        "from glob import glob\n",
        "from tqdm import tqdm\n",
        "from pprint import pprint\n",
        "import torch\n",
        "import pytorch_lightning as pl\n",
        "from rouge import Rouge # ëª¨ë¸ì˜ ì„±ëŠ¥ì„ í‰ê°€í•˜ê¸° ìœ„í•œ ë¼ì´ë¸ŒëŸ¬ë¦¬ì…ë‹ˆë‹¤.\n",
        "\n",
        "from torch.utils.data import Dataset , DataLoader\n",
        "from transformers import AutoTokenizer, BartForConditionalGeneration, BartConfig\n",
        "from transformers import Seq2SeqTrainingArguments, Seq2SeqTrainer\n",
        "from transformers import Trainer, TrainingArguments\n",
        "from transformers import EarlyStoppingCallback\n",
        "\n",
        "import wandb # ëª¨ë¸ í•™ìŠµ ê³¼ì •ì„ ì†ì‰½ê²Œ Trackingí•˜ê³ , ì‹œê°í™”í•  ìˆ˜ ìˆëŠ” ë¼ì´ë¸ŒëŸ¬ë¦¬ì…ë‹ˆë‹¤.\n",
        "\n",
        "\n",
        "from transformers import T5TokenizerFast, T5ForConditionalGeneration, T5Config\n",
        "from transformers import AutoModelForSeq2SeqLM, AutoConfig\n",
        "### 2) Config file ë§Œë“¤ê¸° (ì„ íƒ)\n",
        "- ëª¨ë¸ ìƒì„±ì— í•„ìš”í•œ ë‹¤ì–‘í•œ ë§¤ê°œë³€ìˆ˜ ì •ë³´ë¥¼ ì €ì¥í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.\n",
        "  ë”°ë¼ì„œ, ì½”ë“œ ìƒì—ì„œ ëª¨ë¸ì˜ ë§¤ê°œë³€ìˆ˜ë¥¼ ì„¤ì •í•  ìˆ˜ë„ ìˆì§€ë§Œ ë…ë¦½ì ì¸ ë§¤ê°œë³€ìˆ˜ ì •ë³´ íŒŒì¼ì„ ìƒì„±í•˜ì—¬ ê´€ë¦¬í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.\n",
        "# config ì„¤ì •ì— tokenizer ëª¨ë“ˆì´ ì‚¬ìš©ë˜ë¯€ë¡œ ë¯¸ë¦¬ tokenizerë¥¼ ì •ì˜í•´ì¤ë‹ˆë‹¤.\n",
        "#tokenizer = AutoTokenizer.from_pretrained(\"digit82/kobart-summarization\")\n",
        "\n",
        "model_name = \"lcw99/t5-large-korean-text-summary\"\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "config_data = {\n",
        "    \"general\": {\n",
        "        \"data_path\": \"/root/home/data/\", # ëª¨ë¸ ìƒì„±ì— í•„ìš”í•œ ë°ì´í„° ê²½ë¡œë¥¼ ì‚¬ìš©ì í™˜ê²½ì— ë§ê²Œ ì§€ì •í•©ë‹ˆë‹¤.\n",
        "        \"model_name\": f\"{model_name}\", # ë¶ˆëŸ¬ì˜¬ ëª¨ë¸ì˜ ì´ë¦„ì„ ì‚¬ìš©ì í™˜ê²½ì— ë§ê²Œ ì§€ì •í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.\n",
        "        \"output_dir\": \"/root/home/data/\" # ëª¨ë¸ì˜ ìµœì¢… ì¶œë ¥ ê°’ì„ ì €ì¥í•  ê²½ë¡œë¥¼ ì„¤ì •í•©ë‹ˆë‹¤.\n",
        "    },\n",
        "    \"tokenizer\": {\n",
        "        \"encoder_max_len\": 512,\n",
        "        \"decoder_max_len\": 100,\n",
        "        \"bos_token\": f\"{tokenizer.bos_token}\",\n",
        "        \"eos_token\": f\"{tokenizer.eos_token}\",\n",
        "        # íŠ¹ì • ë‹¨ì–´ë“¤ì´ ë¶„í•´ë˜ì–´ tokenizationì´ ìˆ˜í–‰ë˜ì§€ ì•Šë„ë¡ special_tokensì„ ì§€ì •í•´ì¤ë‹ˆë‹¤.\n",
        "        \"special_tokens\": ['#Person1#', '#Person2#', '#Person3#', '#PhoneNumber#', '#Address#', '#PassportNumber#']\n",
        "    },\n",
        "    \"training\": {\n",
        "        \"overwrite_output_dir\": True,\n",
        "        \"num_train_epochs\": 20,\n",
        "        \"learning_rate\": 1e-5,\n",
        "        \"per_device_train_batch_size\": 4, #50, - out of memory\n",
        "        \"per_device_eval_batch_size\": 3, #32, - out of memory\n",
        "        \"warmup_ratio\": 0.1,\n",
        "        \"weight_decay\": 0.01,\n",
        "        \"lr_scheduler_type\": 'cosine',\n",
        "        \"optim\": 'adamw_torch',\n",
        "        \"gradient_accumulation_steps\": 1,\n",
        "        \"evaluation_strategy\": 'epoch',\n",
        "        \"save_strategy\": 'epoch',\n",
        "        \"save_total_limit\": 5,\n",
        "        \"fp16\": True,\n",
        "        \"load_best_model_at_end\": True,\n",
        "        \"seed\": 42,\n",
        "        \"logging_dir\": \"./logs\",\n",
        "        \"logging_strategy\": \"epoch\",\n",
        "        \"predict_with_generate\": True,\n",
        "        \"generation_max_length\": 100,\n",
        "        \"do_train\": True,\n",
        "        \"do_eval\": True,\n",
        "        \"early_stopping_patience\": 5, #3,\n",
        "        \"early_stopping_threshold\": 0.001,\n",
        "        \"report_to\": \"wandb\" # (ì„ íƒ) wandbë¥¼ ì‚¬ìš©í•  ë•Œ ì„¤ì •í•©ë‹ˆë‹¤.\n",
        "    },\n",
        "    # (ì„ íƒ) wandb í™ˆí˜ì´ì§€ì— ê°€ì…í•˜ì—¬ ì–»ì€ ì •ë³´ë¥¼ ê¸°ë°˜ìœ¼ë¡œ ì‘ì„±í•©ë‹ˆë‹¤.\n",
        "    \"wandb\": {\n",
        "        \"entity\": \"newwhy2\",\n",
        "        \"project\": \"NLP-project\",\n",
        "        \"name\": \"BaseLineCode-2\"\n",
        "    },\n",
        "    \"inference\": {\n",
        "        \"ckt_path\": \"/root/home/data/ckt/\", # ì‚¬ì „ í•™ìŠµì´ ì§„í–‰ëœ ëª¨ë¸ì˜ checkpointë¥¼ ì €ì¥í•  ê²½ë¡œë¥¼ ì„¤ì •í•©ë‹ˆë‹¤.\n",
        "        \"result_path\": \"/root/home/data/prediction/\",\n",
        "        \"no_repeat_ngram_size\": 2,\n",
        "        \"early_stopping\": True,\n",
        "        \"generate_max_length\": 100,\n",
        "        \"num_beams\": 4,\n",
        "        \"batch_size\" : 4, #32 - out of memory\n",
        "        # ì •í™•í•œ ëª¨ë¸ í‰ê°€ë¥¼ ìœ„í•´ ì œê±°í•  ë¶ˆí•„ìš”í•œ ìƒì„± í† í°ë“¤ì„ ì •ì˜í•©ë‹ˆë‹¤.\n",
        "        \"remove_tokens\": ['<usr>', f\"{tokenizer.bos_token}\", f\"{tokenizer.eos_token}\", f\"{tokenizer.pad_token}\"]\n",
        "    }\n",
        "}\n",
        "- ì°¸ê³ âœ…\n",
        ": wandb ë¼ì´ë¸ŒëŸ¬ë¦¬ë¥¼ ì‚¬ìš©í•˜ê¸° ìœ„í•´ì„  entity, project, nameë¥¼ ì§€ì •í•´ì£¼ì–´ì•¼ í•©ë‹ˆë‹¤. wandb í™ˆí˜ì´ì§€ì— ê°€ì…í•œ í›„ ì–»ì€ ì •ë³´ë¥¼ ì…ë ¥í•˜ì—¬ ì‘ë™í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.\n",
        "# ëª¨ë¸ì˜ êµ¬ì„± ì •ë³´ë¥¼ YAML íŒŒì¼ë¡œ ì €ì¥í•©ë‹ˆë‹¤.\n",
        "config_path = \"/root/home/data/config.yaml\"\n",
        "with open(config_path, \"w\") as file:\n",
        "    yaml.dump(config_data, file, allow_unicode=True)\n",
        "### 3) Configuration ë¶ˆëŸ¬ì˜¤ê¸°\n",
        "# ì €ì¥ëœ config íŒŒì¼ì„ ë¶ˆëŸ¬ì˜µë‹ˆë‹¤.\n",
        "config_path = \"/root/home/data/config.yaml\"\n",
        "\n",
        "with open(config_path, \"r\") as file:\n",
        "    loaded_config = yaml.safe_load(file)\n",
        "\n",
        "# ë¶ˆëŸ¬ì˜¨ config íŒŒì¼ì˜ ì „ì²´ ë‚´ìš©ì„ í™•ì¸í•©ë‹ˆë‹¤.\n",
        "pprint(loaded_config)\n",
        "# ì‹¤í—˜ì— ì“°ì¼ ë°ì´í„°ì˜ ê²½ë¡œ, ì‚¬ìš©ë  ëª¨ë¸, ëª¨ë¸ì˜ ìµœì¢… ì¶œë ¥ ê²°ê³¼ë¥¼ ì €ì¥í•  ê²½ë¡œì— ëŒ€í•´ í™•ì¸í•©ë‹ˆë‹¤.\n",
        "loaded_config['general']\n",
        "# ì´ê³³ì— ì‚¬ìš©ìê°€ ì €ì¥í•œ ë°ì´í„° dir ì„¤ì •í•˜ê¸°\n",
        "loaded_config['general']['data_path'] = \"data_path\"\n",
        "# ë°ì´í„° ì „ì²˜ë¦¬ë¥¼ í•˜ê¸° ìœ„í•´ tokenization ê³¼ì •ì—ì„œ í•„ìš”í•œ ì •ë³´ë“¤ì„ í™•ì¸í•©ë‹ˆë‹¤.\n",
        "loaded_config['tokenizer']\n",
        "# ëª¨ë¸ì´ í›ˆë ¨ ì‹œ ì ìš©ë  ë§¤ê°œë³€ìˆ˜ë¥¼ í™•ì¸í•©ë‹ˆë‹¤.\n",
        "loaded_config['training']\n",
        "# ëª¨ë¸ í•™ìŠµ ê³¼ì •ì— ëŒ€í•œ ì •ë³´ë¥¼ ì œê³µí•´ì£¼ëŠ” wandb ì„¤ì • ë‚´ìš©ì„ í™•ì¸í•©ë‹ˆë‹¤.\n",
        "loaded_config['wandb']\n",
        "# (ì„ íƒ) ì´ê³³ì— ì‚¬ìš©ìê°€ ì‚¬ìš©í•  wandb config ì„¤ì •\n",
        "loaded_config['wandb']['entity'] = \"newwhy2\"\n",
        "loaded_config['wandb']['name'] = \"BaseLineCode-2\"\n",
        "loaded_config['wandb']['project'] = \"NLP-project\"\n",
        "# ëª¨ë¸ì´ ìµœì¢… ê²°ê³¼ë¥¼ ì¶œë ¥í•˜ê¸° ìœ„í•œ ë§¤ê°œë³€ìˆ˜ ì •ë³´ë¥¼ í™•ì¸í•©ë‹ˆë‹¤.\n",
        "loaded_config['inference']\n",
        "### 4) ë°ì´í„° ë¶ˆëŸ¬ì™€ì„œ í™•ì¸í•´ë³´ê¸°\n",
        "- ì‹¤í—˜ì—ì„œ ì“°ì¼ ë°ì´í„°ë¥¼ loadí•˜ì—¬ ë°ì´í„°ì˜ êµ¬ì¡°ì™€ ë‚´ìš©ì„ ì‚´í´ë³´ê² ìŠµë‹ˆë‹¤.\n",
        "- Train, dev, test ìˆœì„œëŒ€ë¡œ 12457, 499, 250ê°œ ì”© ë°ì´í„°ê°€ êµ¬ì„±ë˜ì–´ ìˆìŠµë‹ˆë‹¤.\n",
        "# configì— ì €ì¥ëœ ë°ì´í„° ê²½ë¡œë¥¼ í†µí•´ trainê³¼ validation dataë¥¼ ë¶ˆëŸ¬ì˜µë‹ˆë‹¤.\n",
        "data_path = loaded_config['general']['data_path']\n",
        "\n",
        "# train dataì˜ êµ¬ì¡°ì™€ ë‚´ìš©ì„ í™•ì¸í•©ë‹ˆë‹¤.\n",
        "train_df = pd.read_csv(os.path.join(data_path,'train.csv'))\n",
        "train_df.tail()\n",
        "# validation dataì˜ êµ¬ì¡°ì™€ ë‚´ìš©ì„ í™•ì¸í•©ë‹ˆë‹¤.\n",
        "val_df = pd.read_csv(os.path.join(data_path,'dev.csv'))\n",
        "val_df.tail()\n",
        "## 1. ë°ì´í„° ê°€ê³µ ë° ë°ì´í„°ì…‹ í´ë˜ìŠ¤ êµ¬ì¶•\n",
        "- csv file ì„ ë¶ˆëŸ¬ì™€ì„œ encoder ì™€ decoderì˜ ì…ë ¥í˜•íƒœë¡œ ê°€ê³µí•´ì¤ë‹ˆë‹¤.\n",
        "- ê°€ê³µëœ ë°ì´í„°ë¥¼ torch dataset class ë¡œ êµ¬ì¶•í•˜ì—¬ ëª¨ë¸ì— ì…ë ¥ê°€ëŠ¥í•œ í˜•íƒœë¡œ ë§Œë“­ë‹ˆë‹¤.\n",
        "# ë°ì´í„° ì „ì²˜ë¦¬ë¥¼ ìœ„í•œ í´ë˜ìŠ¤ë¡œ, ë°ì´í„°ì…‹ì„ ë°ì´í„°í”„ë ˆì„ìœ¼ë¡œ ë³€í™˜í•˜ê³  ì¸ì½”ë”ì™€ ë””ì½”ë”ì˜ ì…ë ¥ì„ ìƒì„±í•©ë‹ˆë‹¤.\n",
        "class Preprocess:\n",
        "    def __init__(self,\n",
        "            bos_token: str,\n",
        "            eos_token: str,\n",
        "        ) -> None:\n",
        "\n",
        "        self.bos_token = bos_token\n",
        "        self.eos_token = eos_token\n",
        "\n",
        "    @staticmethod\n",
        "    # ì‹¤í—˜ì— í•„ìš”í•œ ì»¬ëŸ¼ì„ ê°€ì ¸ì˜µë‹ˆë‹¤.\n",
        "    def make_set_as_df(file_path, is_train = True):\n",
        "        if is_train:\n",
        "            df = pd.read_csv(file_path)\n",
        "            train_df = df[['fname','dialogue','summary']]\n",
        "            return train_df\n",
        "        else:\n",
        "            df = pd.read_csv(file_path)\n",
        "            test_df = df[['fname','dialogue']]\n",
        "            return test_df\n",
        "\n",
        "    # BART ëª¨ë¸ì˜ ì…ë ¥, ì¶œë ¥ í˜•íƒœë¥¼ ë§ì¶”ê¸° ìœ„í•´ ì „ì²˜ë¦¬ë¥¼ ì§„í–‰í•©ë‹ˆë‹¤.\n",
        "    def make_input(self, dataset,is_test = False):\n",
        "        if is_test:\n",
        "            encoder_input = dataset['dialogue']\n",
        "            decoder_input = [self.bos_token] * len(dataset['dialogue'])\n",
        "            return encoder_input.tolist(), list(decoder_input)\n",
        "        else:\n",
        "            encoder_input = dataset['dialogue']\n",
        "            decoder_input = dataset['summary'].apply(lambda x : self.bos_token + str(x)) # Ground truthë¥¼ ë””ì½”ë”ì˜ inputìœ¼ë¡œ ì‚¬ìš©í•˜ì—¬ í•™ìŠµí•©ë‹ˆë‹¤.\n",
        "            decoder_output = dataset['summary'].apply(lambda x : str(x) + self.eos_token)\n",
        "            return encoder_input.tolist(), decoder_input.tolist(), decoder_output.tolist()\n",
        "\n",
        "# Trainì— ì‚¬ìš©ë˜ëŠ” Dataset í´ë˜ìŠ¤ë¥¼ ì •ì˜í•©ë‹ˆë‹¤.\n",
        "class DatasetForTrain(Dataset):\n",
        "    def __init__(self, encoder_input, decoder_input, labels, len):\n",
        "        self.encoder_input = encoder_input\n",
        "        self.decoder_input = decoder_input\n",
        "        self.labels = labels\n",
        "        self.len = len\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        item = {key: val[idx].clone().detach() for key, val in self.encoder_input.items()} # item[input_ids], item[attention_mask]\n",
        "        # item2 = {key: val[idx].clone().detach() for key, val in self.decoder_input.items()} # item2[input_ids], item2[attention_mask]\n",
        "        # item2['decoder_input_ids'] = item2['input_ids']\n",
        "        # item2['decoder_attention_mask'] = item2['attention_mask']\n",
        "        # item2.pop('input_ids')\n",
        "        # item2.pop('attention_mask')\n",
        "        # item.update(item2) #item[input_ids], item[attention_mask] item[decoder_input_ids], item[decoder_attention_mask]\n",
        "        item['labels'] = self.labels['input_ids'][idx] #item[input_ids], item[attention_mask] item[decoder_input_ids], item[decoder_attention_mask], item[labels]\n",
        "        return item\n",
        "\n",
        "    def __len__(self):\n",
        "        return self.len\n",
        "\n",
        "# Validationì— ì‚¬ìš©ë˜ëŠ” Dataset í´ë˜ìŠ¤ë¥¼ ì •ì˜í•©ë‹ˆë‹¤.\n",
        "class DatasetForVal(Dataset):\n",
        "    def __init__(self, encoder_input, decoder_input, labels, len):\n",
        "        self.encoder_input = encoder_input\n",
        "        self.decoder_input = decoder_input\n",
        "        self.labels = labels\n",
        "        self.len = len\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        item = {key: val[idx].clone().detach() for key, val in self.encoder_input.items()} # item[input_ids], item[attention_mask]\n",
        "        # item2 = {key: val[idx].clone().detach() for key, val in self.decoder_input.items()} # item2[input_ids], item2[attention_mask]\n",
        "        # item2['decoder_input_ids'] = item2['input_ids']\n",
        "        # item2['decoder_attention_mask'] = item2['attention_mask']\n",
        "        # item2.pop('input_ids')\n",
        "        # item2.pop('attention_mask')\n",
        "        # item.update(item2) #item[input_ids], item[attention_mask] item[decoder_input_ids], item[decoder_attention_mask]\n",
        "        item['labels'] = self.labels['input_ids'][idx] #item[input_ids], item[attention_mask] item[decoder_input_ids], item[decoder_attention_mask], item[labels]\n",
        "        return item\n",
        "\n",
        "    def __len__(self):\n",
        "        return self.len\n",
        "\n",
        "# Testì— ì‚¬ìš©ë˜ëŠ” Dataset í´ë˜ìŠ¤ë¥¼ ì •ì˜í•©ë‹ˆë‹¤.\n",
        "class DatasetForInference(Dataset):\n",
        "    def __init__(self, encoder_input, test_id, len):\n",
        "        self.encoder_input = encoder_input\n",
        "        self.test_id = test_id\n",
        "        self.len = len\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        item = {key: val[idx].clone().detach() for key, val in self.encoder_input.items()}\n",
        "        item['ID'] = self.test_id[idx]\n",
        "        return item\n",
        "\n",
        "    def __len__(self):\n",
        "        return self.len\n",
        "\n",
        "# tokenization ê³¼ì •ê¹Œì§€ ì§„í–‰ëœ ìµœì¢…ì ìœ¼ë¡œ ëª¨ë¸ì— ì…ë ¥ë  ë°ì´í„°ë¥¼ ì¶œë ¥í•©ë‹ˆë‹¤.\n",
        "def prepare_train_dataset(config, preprocessor, data_path, tokenizer):\n",
        "    train_file_path = os.path.join(data_path,'train.csv')\n",
        "    val_file_path = os.path.join(data_path,'dev.csv')\n",
        "\n",
        "    # train, validationì— ëŒ€í•´ ê°ê° ë°ì´í„°í”„ë ˆì„ì„ êµ¬ì¶•í•©ë‹ˆë‹¤.\n",
        "    train_data = preprocessor.make_set_as_df(train_file_path)\n",
        "    val_data = preprocessor.make_set_as_df(val_file_path)\n",
        "\n",
        "    print('-'*150)\n",
        "    print(f'train_data:\\n {train_data[\"dialogue\"][0]}')\n",
        "    print(f'train_label:\\n {train_data[\"summary\"][0]}')\n",
        "\n",
        "    print('-'*150)\n",
        "    print(f'val_data:\\n {val_data[\"dialogue\"][0]}')\n",
        "    print(f'val_label:\\n {val_data[\"summary\"][0]}')\n",
        "\n",
        "    encoder_input_train , decoder_input_train, decoder_output_train = preprocessor.make_input(train_data)\n",
        "    encoder_input_val , decoder_input_val, decoder_output_val = preprocessor.make_input(val_data)\n",
        "    print('-'*10, 'Load data complete', '-'*10,)\n",
        "\n",
        "    tokenized_encoder_inputs = tokenizer(encoder_input_train, return_tensors=\"pt\", padding=True,\n",
        "                            add_special_tokens=True, truncation=True, max_length=config['tokenizer']['encoder_max_len'], return_token_type_ids=False)\n",
        "    tokenized_decoder_inputs = tokenizer(decoder_input_train, return_tensors=\"pt\", padding=True,\n",
        "                        add_special_tokens=True, truncation=True, max_length=config['tokenizer']['decoder_max_len'], return_token_type_ids=False)\n",
        "    tokenized_decoder_ouputs = tokenizer(decoder_output_train, return_tensors=\"pt\", padding=True,\n",
        "                        add_special_tokens=True, truncation=True, max_length=config['tokenizer']['decoder_max_len'], return_token_type_ids=False)\n",
        "\n",
        "    train_inputs_dataset = DatasetForTrain(tokenized_encoder_inputs, tokenized_decoder_inputs, tokenized_decoder_ouputs,len(encoder_input_train))\n",
        "\n",
        "    val_tokenized_encoder_inputs = tokenizer(encoder_input_val, return_tensors=\"pt\", padding=True,\n",
        "                        add_special_tokens=True, truncation=True, max_length=config['tokenizer']['encoder_max_len'], return_token_type_ids=False)\n",
        "    val_tokenized_decoder_inputs = tokenizer(decoder_input_val, return_tensors=\"pt\", padding=True,\n",
        "                        add_special_tokens=True, truncation=True, max_length=config['tokenizer']['decoder_max_len'], return_token_type_ids=False)\n",
        "    val_tokenized_decoder_ouputs = tokenizer(decoder_output_val, return_tensors=\"pt\", padding=True,\n",
        "                        add_special_tokens=True, truncation=True, max_length=config['tokenizer']['decoder_max_len'], return_token_type_ids=False)\n",
        "\n",
        "    val_inputs_dataset = DatasetForVal(val_tokenized_encoder_inputs, val_tokenized_decoder_inputs, val_tokenized_decoder_ouputs,len(encoder_input_val))\n",
        "\n",
        "    print('-'*10, 'Make dataset complete', '-'*10,)\n",
        "    return train_inputs_dataset, val_inputs_dataset\n",
        "## 2. Trainer ë° Trainingargs êµ¬ì¶•í•˜ê¸°\n",
        "- Huggingface ì˜ Trainer ì™€ Training argumentsë¥¼ í™œìš©í•˜ì—¬ ëª¨ë¸ í•™ìŠµì„ ì¼ê´„ì ìœ¼ë¡œ ì²˜ë¦¬í•´ì£¼ëŠ” í´ë˜ìŠ¤ë¥¼ ì •ì˜í•©ë‹ˆë‹¤.\n",
        "# ëª¨ë¸ ì„±ëŠ¥ì— ëŒ€í•œ í‰ê°€ ì§€í‘œë¥¼ ì •ì˜í•©ë‹ˆë‹¤. ë³¸ ëŒ€íšŒì—ì„œëŠ” ROUGE ì ìˆ˜ë¥¼ í†µí•´ ëª¨ë¸ì˜ ì„±ëŠ¥ì„ í‰ê°€í•©ë‹ˆë‹¤.\n",
        "def compute_metrics(config,tokenizer,pred):\n",
        "    rouge = Rouge()\n",
        "    predictions = pred.predictions\n",
        "    labels = pred.label_ids\n",
        "\n",
        "    predictions[predictions == -100] = tokenizer.pad_token_id\n",
        "    labels[labels == -100] = tokenizer.pad_token_id\n",
        "\n",
        "    decoded_preds = tokenizer.batch_decode(predictions, clean_up_tokenization_spaces=True)\n",
        "    labels = tokenizer.batch_decode(labels, clean_up_tokenization_spaces=True)\n",
        "\n",
        "    # ì •í™•í•œ í‰ê°€ë¥¼ ìœ„í•´ ë¯¸ë¦¬ ì •ì˜ëœ ë¶ˆí•„ìš”í•œ ìƒì„±í† í°ë“¤ì„ ì œê±°í•©ë‹ˆë‹¤.\n",
        "    replaced_predictions = decoded_preds.copy()\n",
        "    replaced_labels = labels.copy()\n",
        "    remove_tokens = config['inference']['remove_tokens']\n",
        "    for token in remove_tokens:\n",
        "        replaced_predictions = [sentence.replace(token,\" \") for sentence in replaced_predictions]\n",
        "        replaced_labels = [sentence.replace(token,\" \") for sentence in replaced_labels]\n",
        "\n",
        "    print('-'*150)\n",
        "    print(f\"PRED: {replaced_predictions[0]}\")\n",
        "    print(f\"GOLD: {replaced_labels[0]}\")\n",
        "    print('-'*150)\n",
        "    print(f\"PRED: {replaced_predictions[1]}\")\n",
        "    print(f\"GOLD: {replaced_labels[1]}\")\n",
        "    print('-'*150)\n",
        "    print(f\"PRED: {replaced_predictions[2]}\")\n",
        "    print(f\"GOLD: {replaced_labels[2]}\")\n",
        "\n",
        "    # ìµœì¢…ì ì¸ ROUGE ì ìˆ˜ë¥¼ ê³„ì‚°í•©ë‹ˆë‹¤.\n",
        "    results = rouge.get_scores(replaced_predictions, replaced_labels,avg=True)\n",
        "\n",
        "    # ROUGE ì ìˆ˜ ì¤‘ F-1 scoreë¥¼ í†µí•´ í‰ê°€í•©ë‹ˆë‹¤.\n",
        "    result = {key: value[\"f\"] for key, value in results.items()}\n",
        "    return result\n",
        "# í•™ìŠµì„ ìœ„í•œ trainer í´ë˜ìŠ¤ì™€ ë§¤ê°œë³€ìˆ˜ë¥¼ ì •ì˜í•©ë‹ˆë‹¤.\n",
        "def load_trainer_for_train(config,generate_model,tokenizer,train_inputs_dataset,val_inputs_dataset):\n",
        "    print('-'*10, 'Make training arguments', '-'*10,)\n",
        "    # set training args\n",
        "    training_args = Seq2SeqTrainingArguments(\n",
        "                output_dir=config['general']['output_dir'], # model output directory\n",
        "                overwrite_output_dir=config['training']['overwrite_output_dir'],\n",
        "                num_train_epochs=config['training']['num_train_epochs'],  # total number of training epochs\n",
        "                learning_rate=config['training']['learning_rate'], # learning_rate\n",
        "                per_device_train_batch_size=config['training']['per_device_train_batch_size'], # batch size per device during training\n",
        "                per_device_eval_batch_size=config['training']['per_device_eval_batch_size'],# batch size for evaluation\n",
        "                warmup_ratio=config['training']['warmup_ratio'],  # number of warmup steps for learning rate scheduler\n",
        "                weight_decay=config['training']['weight_decay'],  # strength of weight decay\n",
        "                lr_scheduler_type=config['training']['lr_scheduler_type'],\n",
        "                optim =config['training']['optim'],\n",
        "                gradient_accumulation_steps=config['training']['gradient_accumulation_steps'],\n",
        "                evaluation_strategy=config['training']['evaluation_strategy'], # evaluation strategy to adopt during training\n",
        "                save_strategy =config['training']['save_strategy'],\n",
        "                save_total_limit=config['training']['save_total_limit'], # number of total save model.\n",
        "                fp16=config['training']['fp16'],\n",
        "                load_best_model_at_end=config['training']['load_best_model_at_end'], # ìµœì¢…ì ìœ¼ë¡œ ê°€ì¥ ë†’ì€ ì ìˆ˜ ì €ì¥\n",
        "                seed=config['training']['seed'],\n",
        "                logging_dir=config['training']['logging_dir'], # directory for storing logs\n",
        "                logging_strategy=config['training']['logging_strategy'],\n",
        "                predict_with_generate=config['training']['predict_with_generate'], #To use BLEU or ROUGE score\n",
        "                generation_max_length=config['training']['generation_max_length'],\n",
        "                do_train=config['training']['do_train'],\n",
        "                do_eval=config['training']['do_eval'],\n",
        "                report_to=config['training']['report_to'] # (ì„ íƒ) wandbë¥¼ ì‚¬ìš©í•  ë•Œ ì„¤ì •í•©ë‹ˆë‹¤.\n",
        "            )\n",
        "\n",
        "    # (ì„ íƒ) ëª¨ë¸ì˜ í•™ìŠµ ê³¼ì •ì„ ì¶”ì í•˜ëŠ” wandbë¥¼ ì‚¬ìš©í•˜ê¸° ìœ„í•´ ì´ˆê¸°í™” í•´ì¤ë‹ˆë‹¤.\n",
        "    wandb.init(\n",
        "        entity=config['wandb']['entity'],\n",
        "        project=config['wandb']['project'],\n",
        "        name=config['wandb']['name'],\n",
        "    )\n",
        "\n",
        "    # (ì„ íƒ) ëª¨ë¸ checkpointë¥¼ wandbì— ì €ì¥í•˜ë„ë¡ í™˜ê²½ ë³€ìˆ˜ë¥¼ ì„¤ì •í•©ë‹ˆë‹¤.\n",
        "    os.environ[\"WANDB_LOG_MODEL\"]=\"true\"\n",
        "    os.environ[\"WANDB_WATCH\"]=\"false\"\n",
        "\n",
        "    # Validation lossê°€ ë” ì´ìƒ ê°œì„ ë˜ì§€ ì•Šì„ ë•Œ í•™ìŠµì„ ì¤‘ë‹¨ì‹œí‚¤ëŠ” EarlyStopping ê¸°ëŠ¥ì„ ì‚¬ìš©í•©ë‹ˆë‹¤.\n",
        "    MyCallback = EarlyStoppingCallback(\n",
        "        early_stopping_patience=config['training']['early_stopping_patience'],\n",
        "        early_stopping_threshold=config['training']['early_stopping_threshold']\n",
        "    )\n",
        "    print('-'*10, 'Make training arguments complete', '-'*10,)\n",
        "    print('-'*10, 'Make trainer', '-'*10,)\n",
        "\n",
        "    # Trainer í´ë˜ìŠ¤ë¥¼ ì •ì˜í•©ë‹ˆë‹¤.\n",
        "    trainer = Seq2SeqTrainer(\n",
        "        model=generate_model, # ì‚¬ìš©ìê°€ ì‚¬ì „ í•™ìŠµí•˜ê¸° ìœ„í•´ ì‚¬ìš©í•  ëª¨ë¸ì„ ì…ë ¥í•©ë‹ˆë‹¤.\n",
        "        args=training_args,\n",
        "        train_dataset=train_inputs_dataset,\n",
        "        eval_dataset=val_inputs_dataset,\n",
        "        compute_metrics = lambda pred: compute_metrics(config,tokenizer, pred),\n",
        "        callbacks = [MyCallback]\n",
        "    )\n",
        "    print('-'*10, 'Make trainer complete', '-'*10,)\n",
        "\n",
        "    return trainer\n",
        "# í•™ìŠµì„ ìœ„í•œ tokenizerì™€ ì‚¬ì „ í•™ìŠµëœ ëª¨ë¸ì„ ë¶ˆëŸ¬ì˜µë‹ˆë‹¤.\n",
        "def load_tokenizer_and_model_for_train(config,device):\n",
        "    print('-'*10, 'Load tokenizer & model', '-'*10,)\n",
        "    print('-'*10, f'Model Name : {config[\"general\"][\"model_name\"]}', '-'*10,)\n",
        "    model_name = config['general']['model_name']\n",
        "    T5_config = T5Config.from_pretrained(model_name)\n",
        "    tokenizer = T5TokenizerFast.from_pretrained(model_name)\n",
        "    generate_model = T5ForConditionalGeneration.from_pretrained(config['general']['model_name'],config=T5_config)\n",
        "\n",
        "    special_tokens_dict={'additional_special_tokens':config['tokenizer']['special_tokens']}\n",
        "    tokenizer.add_special_tokens(special_tokens_dict)\n",
        "\n",
        "    generate_model.resize_token_embeddings(len(tokenizer)) # ì‚¬ì „ì— special tokenì„ ì¶”ê°€í–ˆìœ¼ë¯€ë¡œ ì¬êµ¬ì„± í•´ì¤ë‹ˆë‹¤.\n",
        "    generate_model.to(device)\n",
        "    print(generate_model.config)\n",
        "\n",
        "    print('-'*10, 'Load tokenizer & model complete', '-'*10,)\n",
        "    return generate_model , tokenizer\n",
        "## 3. ëª¨ë¸ í•™ìŠµí•˜ê¸°\n",
        "- ì•ì—ì„œ êµ¬ì¶•í•œ í´ë˜ìŠ¤ ë° í•¨ìˆ˜ë¥¼ í™œìš©í•˜ì—¬ í•™ìŠµ ì§„í–‰í•©ë‹ˆë‹¤.\n",
        "def main(config):\n",
        "    # ì‚¬ìš©í•  deviceë¥¼ ì •ì˜í•©ë‹ˆë‹¤.\n",
        "    device = torch.device('cuda:0' if torch.cuda.is_available()  else 'cpu')\n",
        "    print('-'*10, f'device : {device}', '-'*10,)\n",
        "    print(torch.__version__)\n",
        "\n",
        "    # ì‚¬ìš©í•  ëª¨ë¸ê³¼ tokenizerë¥¼ ë¶ˆëŸ¬ì˜µë‹ˆë‹¤.\n",
        "    generate_model , tokenizer = load_tokenizer_and_model_for_train(config,device)\n",
        "    print('-'*10,\"tokenizer special tokens : \",tokenizer.special_tokens_map,'-'*10)\n",
        "\n",
        "    # í•™ìŠµì— ì‚¬ìš©í•  ë°ì´í„°ì…‹ì„ ë¶ˆëŸ¬ì˜µë‹ˆë‹¤.\n",
        "    preprocessor = Preprocess(config['tokenizer']['bos_token'], config['tokenizer']['eos_token']) # decoder_start_token: str, eos_token: str\n",
        "    data_path = config['general']['data_path']\n",
        "    train_inputs_dataset, val_inputs_dataset = prepare_train_dataset(config,preprocessor, data_path, tokenizer)\n",
        "\n",
        "    # Trainer í´ë˜ìŠ¤ë¥¼ ë¶ˆëŸ¬ì˜µë‹ˆë‹¤.\n",
        "    trainer = load_trainer_for_train(config, generate_model,tokenizer,train_inputs_dataset,val_inputs_dataset)\n",
        "    trainer.train()   # ëª¨ë¸ í•™ìŠµì„ ì‹œì‘í•©ë‹ˆë‹¤.\n",
        "\n",
        "    # (ì„ íƒ) ëª¨ë¸ í•™ìŠµì´ ì™„ë£Œëœ í›„ wandbë¥¼ ì¢…ë£Œí•©ë‹ˆë‹¤.\n",
        "    wandb.finish()\n",
        "!pip show accelerate\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main(loaded_config)\n",
        "## 4. ëª¨ë¸ ì¶”ë¡ í•˜ê¸°\n",
        "# ì´ê³³ì— ë‚´ê°€ ì‚¬ìš©í•  wandb config ì„¤ì •\n",
        "loaded_config['inference']['ckt_path'] = \"./checkpoint-31150\" #\"ì¶”ë¡ ì— ì‚¬ìš©í•  ckt ê²½ë¡œ ì„¤ì •\"\n",
        "- test dataë¥¼ ì‚¬ìš©í•˜ì—¬ ëª¨ë¸ì˜ ì„±ëŠ¥ì„ í™•ì¸í•©ë‹ˆë‹¤.\n",
        "# tokenization ê³¼ì •ê¹Œì§€ ì§„í–‰ëœ ìµœì¢…ì ìœ¼ë¡œ ëª¨ë¸ì— ì…ë ¥ë  ë°ì´í„°ë¥¼ ì¶œë ¥í•©ë‹ˆë‹¤.\n",
        "def prepare_test_dataset(config,preprocessor, tokenizer):\n",
        "\n",
        "    test_file_path = os.path.join(config['general']['data_path'],'test.csv')\n",
        "\n",
        "    test_data = preprocessor.make_set_as_df(test_file_path,is_train=False)\n",
        "    test_id = test_data['fname']\n",
        "\n",
        "    print('-'*150)\n",
        "    print(f'test_data:\\n{test_data[\"dialogue\"][0]}')\n",
        "    print('-'*150)\n",
        "\n",
        "    encoder_input_test , decoder_input_test = preprocessor.make_input(test_data,is_test=True)\n",
        "    print('-'*10, 'Load data complete', '-'*10,)\n",
        "\n",
        "    test_tokenized_encoder_inputs = tokenizer(encoder_input_test, return_tensors=\"pt\", padding=True,\n",
        "                    add_special_tokens=True, truncation=True, max_length=config['tokenizer']['encoder_max_len'], return_token_type_ids=False,)\n",
        "    test_tokenized_decoder_inputs = tokenizer(decoder_input_test, return_tensors=\"pt\", padding=True,\n",
        "                    add_special_tokens=True, truncation=True, max_length=config['tokenizer']['decoder_max_len'], return_token_type_ids=False,)\n",
        "\n",
        "    test_encoder_inputs_dataset = DatasetForInference(test_tokenized_encoder_inputs, test_id, len(encoder_input_test))\n",
        "    print('-'*10, 'Make dataset complete', '-'*10,)\n",
        "\n",
        "    return test_data, test_encoder_inputs_dataset\n",
        "# ì¶”ë¡ ì„ ìœ„í•œ tokenizerì™€ í•™ìŠµì‹œí‚¨ ëª¨ë¸ì„ ë¶ˆëŸ¬ì˜µë‹ˆë‹¤.\n",
        "def load_tokenizer_and_model_for_test(config,device):\n",
        "    print('-'*10, 'Load tokenizer & model', '-'*10,)\n",
        "\n",
        "    model_name = config['general']['model_name']\n",
        "    ckt_path = config['inference']['ckt_path']\n",
        "    print('-'*10, f'Model Name : {model_name}', '-'*10,)\n",
        "    tokenizer = T5TokenizerFast.from_pretrained(model_name)\n",
        "    special_tokens_dict = {'additional_special_tokens': config['tokenizer']['special_tokens']}\n",
        "    tokenizer.add_special_tokens(special_tokens_dict)\n",
        "\n",
        "    generate_model = T5ForConditionalGeneration.from_pretrained(ckt_path)\n",
        "    generate_model.resize_token_embeddings(len(tokenizer))\n",
        "    generate_model.to(device)\n",
        "    print('-'*10, 'Load tokenizer & model complete', '-'*10,)\n",
        "\n",
        "    return generate_model , tokenizer\n",
        "# í•™ìŠµëœ ëª¨ë¸ì´ ìƒì„±í•œ ìš”ì•½ë¬¸ì˜ ì¶œë ¥ ê²°ê³¼ë¥¼ ë³´ì—¬ì¤ë‹ˆë‹¤.\n",
        "def inference(config):\n",
        "    device = torch.device('cuda:0' if torch.cuda.is_available()  else 'cpu')\n",
        "    print('-'*10, f'device : {device}', '-'*10,)\n",
        "    print(torch.__version__)\n",
        "\n",
        "    generate_model , tokenizer = load_tokenizer_and_model_for_test(config,device)\n",
        "\n",
        "    data_path = config['general']['data_path']\n",
        "    preprocessor = Preprocess(config['tokenizer']['bos_token'], config['tokenizer']['eos_token'])\n",
        "\n",
        "    test_data, test_encoder_inputs_dataset = prepare_test_dataset(config,preprocessor, tokenizer)\n",
        "    dataloader = DataLoader(test_encoder_inputs_dataset, batch_size=config['inference']['batch_size'])\n",
        "\n",
        "    summary = []\n",
        "    text_ids = []\n",
        "    with torch.no_grad():\n",
        "        for item in tqdm(dataloader):\n",
        "            text_ids.extend(item['ID'])\n",
        "            generated_ids = generate_model.generate(input_ids=item['input_ids'].to('cuda:0'),\n",
        "                            no_repeat_ngram_size=config['inference']['no_repeat_ngram_size'],\n",
        "                            early_stopping=config['inference']['early_stopping'],\n",
        "                            max_length=config['inference']['generate_max_length'],\n",
        "                            num_beams=config['inference']['num_beams'],\n",
        "                        )\n",
        "            for ids in generated_ids:\n",
        "                result = tokenizer.decode(ids)\n",
        "                summary.append(result)\n",
        "\n",
        "    # ì •í™•í•œ í‰ê°€ë¥¼ ìœ„í•˜ì—¬ ë…¸ì´ì¦ˆì— í•´ë‹¹ë˜ëŠ” ìŠ¤í˜ì…œ í† í°ì„ ì œê±°í•©ë‹ˆë‹¤.\n",
        "    remove_tokens = config['inference']['remove_tokens']\n",
        "    preprocessed_summary = summary.copy()\n",
        "    for token in remove_tokens:\n",
        "        preprocessed_summary = [sentence.replace(token,\" \") for sentence in preprocessed_summary]\n",
        "\n",
        "    output = pd.DataFrame(\n",
        "        {\n",
        "            \"fname\": test_data['fname'],\n",
        "            \"summary\" : preprocessed_summary,\n",
        "        }\n",
        "    )\n",
        "    result_path = config['inference']['result_path']\n",
        "    if not os.path.exists(result_path):\n",
        "        os.makedirs(result_path)\n",
        "    output.to_csv(os.path.join(result_path, \"output.csv\"), index=False)\n",
        "\n",
        "    return output\n",
        "# í•™ìŠµëœ ëª¨ë¸ì˜ testë¥¼ ì§„í–‰í•©ë‹ˆë‹¤.\n",
        "if __name__ == \"__main__\":\n",
        "    output = inference(loaded_config)\n",
        "output  # ê° ëŒ€í™”ë¬¸ì— ëŒ€í•œ ìš”ì•½ë¬¸ì´ ì¶œë ¥ë¨ì„ í™•ì¸í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.\n",
        "import pandas as pd\n",
        "\n",
        "# ì˜ˆì¸¡ íŒŒì¼ ë¶ˆëŸ¬ì˜¤ê¸°\n",
        "pred1 = pd.read_csv(\"output (5).csv\")\n",
        "pred2 = pd.read_csv(\"output (6).csv\")\n",
        "pred3 = pd.read_csv(\"output (7).csv\")\n",
        "pred4 = pd.read_csv(\"output (8).csv\")\n",
        "pred5 = pd.read_csv(\"output (9).csv\")\n",
        "\n",
        "# ì˜ˆì¸¡ ê²°ê³¼ë¥¼ ë¦¬ìŠ¤íŠ¸ë¡œ ì €ì¥\n",
        "predictions = [pred1['summary'], pred2['summary'], pred3['summary'], pred4['summary'], pred5['summary']]\n",
        "\n",
        "# ë°ì´í„°í”„ë ˆì„ìœ¼ë¡œ ë³€í™˜í•˜ì—¬ ê° rowë§ˆë‹¤ í•˜ë“œë³´íŒ…ì„ ì ìš©\n",
        "pred_df = pd.DataFrame(predictions).T  # í–‰(row)ì€ ê° ìƒ˜í”Œ, ì—´(column)ì€ ê° ëª¨ë¸ì˜ ì˜ˆì¸¡ê°’\n",
        "\n",
        "# í•˜ë“œ ë³´íŒ…ì„ ì ìš©\n",
        "def hard_voting(row):\n",
        "    return row.value_counts().idxmax()\n",
        "\n",
        "final_predictions = pred_df.apply(hard_voting, axis=1)\n",
        "\n",
        "# ê²°ê³¼ë¥¼ DataFrameìœ¼ë¡œ ì €ì¥\n",
        "output = pd.DataFrame({'summary': final_predictions})\n",
        "\n",
        "# output.csv íŒŒì¼ë¡œ ì €ì¥\n",
        "output.to_csv(\"output.csv\", index=False)\n",
        "\n",
        "print(\"output.csv íŒŒì¼ì´ ìƒì„±ë˜ì—ˆìŠµë‹ˆë‹¤.\")\n",
        "\n",
        "# CSV íŒŒì¼ ë¶ˆëŸ¬ì˜¤ê¸°\n",
        "pred1 = pd.read_csv(\"output (9).csv\")\n",
        "pred2 = pd.read_csv(\"output (8).csv\")\n",
        "pred3 = pd.read_csv(\"output (7).csv\")\n",
        "pred4 = pd.read_csv(\"output (6).csv\")\n",
        "pred5 = pd.read_csv(\"output (5).csv\")\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# ê° ì˜ˆì¸¡ ê²°ê³¼ë¥¼ ë¦¬ìŠ¤íŠ¸ë¡œ ì €ì¥\n",
        "predictions = [pred1['target'], pred2['target'], pred3['target'], pred4['target'], pred5['target']]\n",
        "\n",
        "# íˆ¬í‘œë¥¼ í†µí•´ ìµœì¢… ì˜ˆì¸¡ ê²°ì •\n",
        "final_predictions = []\n",
        "\n",
        "# ê° ìƒ˜í”Œì— ëŒ€í•´ íˆ¬í‘œ\n",
        "for preds in zip(*predictions):\n",
        "    vote_result = Counter(preds).most_common(1)[0][0]\n",
        "    final_predictions.append(vote_result)\n",
        "\n",
        "# ìµœì¢… ì˜ˆì¸¡ ê²°ê³¼ë¥¼ ë°ì´í„°í”„ë ˆì„ìœ¼ë¡œ ì €ì¥\n",
        "submission = pd.DataFrame({'ID': pred1['ID'], 'target': final_predictions})\n",
        "\n",
        "# ê²°ê³¼ë¥¼ CSV íŒŒì¼ë¡œ ì €ì¥\n",
        "submission.to_csv(\"ensemble_voting_submission.csv\", index=False)\n",
        "\n",
        "import pandas as pd\n",
        "\n",
        "# ì˜ˆì¸¡ íŒŒì¼ ë¶ˆëŸ¬ì˜¤ê¸°\n",
        "pred1 = pd.read_csv(\"output (16).csv\")\n",
        "pred2 = pd.read_csv(\"output (17).csv\")\n",
        "pred3 = pd.read_csv(\"output ().csv\")\n",
        "pred4 = pd.read_csv(\"output (8).csv\")\n",
        "pred5 = pd.read_csv(\"output (9).csv\")\n",
        "\n",
        "# ì˜ˆì¸¡ ê²°ê³¼ë¥¼ ë¦¬ìŠ¤íŠ¸ë¡œ ì €ì¥\n",
        "predictions = [pred1['summary'], pred2['summary'], pred3['summary'], pred4['summary'], pred5['summary']]\n",
        "\n",
        "# ë°ì´í„°í”„ë ˆì„ìœ¼ë¡œ ë³€í™˜í•˜ì—¬ ê° rowë§ˆë‹¤ í•˜ë“œë³´íŒ…ì„ ì ìš©\n",
        "pred_df = pd.DataFrame(predictions).T  # í–‰(row)ì€ ê° ìƒ˜í”Œ, ì—´(column)ì€ ê° ëª¨ë¸ì˜ ì˜ˆì¸¡ê°’\n",
        "\n",
        "# í•˜ë“œ ë³´íŒ…ì„ ì ìš©\n",
        "def hard_voting(row):\n",
        "    return row.value_counts().idxmax()\n",
        "\n",
        "final_predictions = pred_df.apply(hard_voting, axis=1)\n",
        "\n",
        "# ê²°ê³¼ë¥¼ DataFrameìœ¼ë¡œ ì €ì¥\n",
        "output = pd.DataFrame({'summary': final_predictions})\n",
        "\n",
        "# output.csv íŒŒì¼ë¡œ ì €ì¥\n",
        "output.to_csv(\"output.csv\", index=False)\n",
        "\n",
        "print(\"output.csv íŒŒì¼ì´ ìƒì„±ë˜ì—ˆìŠµë‹ˆë‹¤.\")\n",
        "\n",
        "import pandas as pd\n",
        "\n",
        "# ì˜ˆì¸¡ íŒŒì¼ ë¶ˆëŸ¬ì˜¤ê¸°\n",
        "pred1 = pd.read_csv(\"output (16).csv\")\n",
        "pred2 = pd.read_csv(\"output (17).csv\")\n",
        "pred3 = pd.read_csv(\"output (18).csv\")\n",
        "pred4 = pd.read_csv(\"output (19).csv\")\n",
        "pred5 = pd.read_csv(\"output (20).csv\")\n",
        "\n",
        "# ê° íŒŒì¼ì˜ í–‰ ìˆ˜ í™•ì¸ (499ê°œì˜ í–‰ì´ í™•ì¸ë¨)\n",
        "print(len(pred1), len(pred2), len(pred3), len(pred4), len(pred5))\n",
        "\n",
        "# ì˜ˆì¸¡ ê²°ê³¼ë¥¼ ë¦¬ìŠ¤íŠ¸ë¡œ ì €ì¥\n",
        "predictions = [pred1['summary'], pred2['summary'], pred3['summary'], pred4['summary'], pred5['summary']]\n",
        "\n",
        "# ë°ì´í„°í”„ë ˆì„ìœ¼ë¡œ ë³€í™˜ (transpose)\n",
        "pred_df = pd.DataFrame(predictions).T  # í–‰(row)ì€ ê° ìƒ˜í”Œ, ì—´(column)ì€ ê° ëª¨ë¸ì˜ ì˜ˆì¸¡ê°’\n",
        "\n",
        "# ì†Œí”„íŠ¸ë³´íŒ… í‰ë‚´ë‚´ê¸°: ê° ì˜ˆì¸¡ ë‹¨ì–´ì˜ ê°€ì¤‘ì¹˜ë¥¼ í•©ì‚°\n",
        "def soft_voting(row):\n",
        "    word_counts = row.value_counts()  # ê° ë‹¨ì–´ì˜ ë¹ˆë„ë¥¼ ê³„ì‚°\n",
        "    return word_counts.idxmax()  # ê°€ì¥ ë¹ˆë„ê°€ ë†’ì€ ë‹¨ì–´ë¥¼ ì„ íƒ\n",
        "\n",
        "# ìµœì¢… ì˜ˆì¸¡ê°’\n",
        "final_predictions = pred_df.apply(soft_voting, axis=1)\n",
        "\n",
        "# 'fname' ì—´ ìƒì„± (test_0ë¶€í„° test_498ê¹Œì§€)\n",
        "fname = [f\"test_{i}\" for i in range(len(final_predictions))]  # ë°ì´í„° ê¸¸ì´ì— ë§ê²Œ ìƒì„±\n",
        "\n",
        "# ê²°ê³¼ë¥¼ DataFrameìœ¼ë¡œ ì €ì¥\n",
        "output = pd.DataFrame({'fname': fname, 'summary': final_predictions})\n",
        "\n",
        "# í–‰ ìˆ˜ê°€ 499ê°œì¸ì§€ í™•ì¸\n",
        "print(\"Output row count:\", len(output))\n",
        "\n",
        "# output.csv íŒŒì¼ì„ UTF-8 ì¸ì½”ë”©ìœ¼ë¡œ ì €ì¥\n",
        "output.to_csv(\"output.csv\", index=False, encoding='utf-8')\n",
        "\n",
        "print(\"output.csv íŒŒì¼ì´ UTF-8 í˜•ì‹ìœ¼ë¡œ ìƒì„±ë˜ì—ˆìŠµë‹ˆë‹¤.\")\n",
        "\n",
        "import pandas as pd\n",
        "\n",
        "# output.csv íŒŒì¼ ë¶ˆëŸ¬ì˜¤ê¸°\n",
        "df = pd.read_csv(\"output.csv\")\n",
        "\n",
        "# ì²« ë²ˆì§¸ ì—´ì˜ ê°’ ì¶”ì¶œ\n",
        "test_ids = df.iloc[:, 0]\n",
        "\n",
        "# ëª¨ë“  ìˆ«ìë¥¼ í¬í•¨í•˜ëŠ” ì„¸íŠ¸ ìƒì„±\n",
        "expected_ids = set(f\"test_{i}\" for i in range(500))\n",
        "\n",
        "# ì‹¤ì œë¡œ ì¡´ì¬í•˜ëŠ” ì„¸íŠ¸ ìƒì„±\n",
        "actual_ids = set(test_ids)\n",
        "\n",
        "# ë¹ ì§„ ìˆ«ì ì°¾ê¸°\n",
        "missing_ids = expected_ids - actual_ids\n",
        "\n",
        "# ë¹ ì§„ ìˆ«ìë“¤ì„ ì •ë ¬í•˜ì—¬ ë°°ì—´\n",
        "missing_ids_sorted = sorted(missing_ids, key=lambda x: int(x.split('_')[1]))\n",
        "\n",
        "# ì¶œë ¥\n",
        "print(\"ë¹ ì§„ ìˆ«ìë“¤:\")\n",
        "for missing_id in missing_ids_sorted:\n",
        "    print(missing_id)\n",
        "\n",
        "# ë¹ ì§„ ìˆ«ìë“¤ì„ ë¦¬ìŠ¤íŠ¸ë¡œ ë°˜í™˜\n",
        "missing_ids_sorted\n",
        "\n",
        "import pandas as pd\n",
        "\n",
        "# íŒŒì¼ ì´ë¦„ ë¦¬ìŠ¤íŠ¸\n",
        "file_names = [f\"output ({i}).csv\" for i in range(16, 26)]\n",
        "\n",
        "# ì˜ˆì¸¡ ê²°ê³¼ë¥¼ ì €ì¥í•  ë¦¬ìŠ¤íŠ¸\n",
        "predictions = []\n",
        "\n",
        "# ê° íŒŒì¼ì„ ë¶ˆëŸ¬ì™€ì„œ ì˜ˆì¸¡ ê²°ê³¼ë¥¼ ë¦¬ìŠ¤íŠ¸ì— ì €ì¥\n",
        "for file_name in file_names:\n",
        "    df = pd.read_csv(file_name)\n",
        "    predictions.append(df['summary'])\n",
        "\n",
        "# ë°ì´í„°í”„ë ˆì„ìœ¼ë¡œ ë³€í™˜í•˜ì—¬ ê° rowë§ˆë‹¤ í•˜ë“œ ë³´íŒ…ì„ ì ìš©\n",
        "pred_df = pd.DataFrame(predictions).T  # í–‰(row)ì€ ê° ìƒ˜í”Œ, ì—´(column)ì€ ê° ëª¨ë¸ì˜ ì˜ˆì¸¡ê°’\n",
        "\n",
        "# í•˜ë“œ ë³´íŒ…ì„ ì ìš©\n",
        "def hard_voting(row):\n",
        "    return row.value_counts().idxmax()\n",
        "\n",
        "final_predictions = pred_df.apply(hard_voting, axis=1)\n",
        "\n",
        "# ê²°ê³¼ë¥¼ DataFrameìœ¼ë¡œ ì €ì¥\n",
        "output = pd.DataFrame({'summary': final_predictions})\n",
        "\n",
        "# í–‰ ì´ë¦„ ì§€ì •\n",
        "num_rows = len(output)\n",
        "row_names = [f'test_{i}' for i in range(num_rows)]\n",
        "\n",
        "# 466ë²ˆì§¸ í–‰ì€ ë¹„ìš°ê³ , 467ë²ˆì§¸ í–‰ë¶€í„° ì—°ì†ì ìœ¼ë¡œ ì´ë¦„ì„ ì„¤ì •\n",
        "for i in range(num_rows):\n",
        "    if i < 466:\n",
        "        row_names[i] = f'test_{i}'\n",
        "    elif i == 466:\n",
        "        row_names[i] = 'test_467'\n",
        "    elif i > 466:\n",
        "        row_names[i] = f'test_{i + 1}'\n",
        "\n",
        "output.index = row_names\n",
        "\n",
        "# CSV íŒŒì¼ë¡œ ì €ì¥ (UTF-8 ì¸ì½”ë”©)\n",
        "output.to_csv(\"output.csv\", index_label='fname', encoding='utf-8')\n",
        "\n",
        "print(\"output.csv íŒŒì¼ì´ ìƒì„±ë˜ì—ˆìŠµë‹ˆë‹¤.\")\n",
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "# íŒŒì¼ ì´ë¦„ ë¦¬ìŠ¤íŠ¸\n",
        "file_names = [f\"output ({i}).csv\" for i in range(16, 26)]\n",
        "\n",
        "# ì˜ˆì¸¡ ê²°ê³¼ë¥¼ ì €ì¥í•  ë¦¬ìŠ¤íŠ¸\n",
        "predictions = []\n",
        "\n",
        "# ê° íŒŒì¼ì„ ë¶ˆëŸ¬ì™€ì„œ ì˜ˆì¸¡ ê²°ê³¼ë¥¼ ë¦¬ìŠ¤íŠ¸ì— ì €ì¥\n",
        "for file_name in file_names:\n",
        "    df = pd.read_csv(file_name)\n",
        "    predictions.append(df['summary'])\n",
        "\n",
        "# ë°ì´í„°í”„ë ˆì„ìœ¼ë¡œ ë³€í™˜\n",
        "pred_df = pd.DataFrame(predictions).T\n",
        "\n",
        "# ì†Œí”„íŠ¸ ë³´íŒ… í•¨ìˆ˜ ì •ì˜\n",
        "def soft_voting(row):\n",
        "    # ê° ì˜ˆì¸¡ì˜ ë¹ˆë„ë¥¼ ê³„ì‚°\n",
        "    value_counts = row.value_counts()\n",
        "\n",
        "    # ì „ì²´ ì˜ˆì¸¡ ìˆ˜\n",
        "    total_predictions = len(row)\n",
        "\n",
        "    # ê° ì˜ˆì¸¡ì˜ ë¹ˆë„ë¥¼ í™•ë¥ ë¡œ ë³€í™˜\n",
        "    probabilities = value_counts / total_predictions\n",
        "\n",
        "    # ê°€ì¥ ë†’ì€ í™•ë¥ ì„ ê°€ì§„ ì˜ˆì¸¡ ì„ íƒ\n",
        "    return probabilities.index[0]\n",
        "\n",
        "# ì†Œí”„íŠ¸ ë³´íŒ… ì ìš©\n",
        "final_predictions = pred_df.apply(soft_voting, axis=1)\n",
        "\n",
        "# ê²°ê³¼ë¥¼ DataFrameìœ¼ë¡œ ì €ì¥\n",
        "output = pd.DataFrame({'summary': final_predictions})\n",
        "\n",
        "# í–‰ ì´ë¦„ ì§€ì •\n",
        "num_rows = len(output)\n",
        "row_names = [f'test_{i}' for i in range(num_rows)]\n",
        "\n",
        "# 466ë²ˆì§¸ í–‰ì€ ë¹„ìš°ê³ , 467ë²ˆì§¸ í–‰ë¶€í„° ì—°ì†ì ìœ¼ë¡œ ì´ë¦„ì„ ì„¤ì •\n",
        "for i in range(num_rows):\n",
        "    if i < 466:\n",
        "        row_names[i] = f'test_{i}'\n",
        "    elif i == 466:\n",
        "        row_names[i] = 'test_467'\n",
        "    elif i > 466:\n",
        "        row_names[i] = f'test_{i + 1}'\n",
        "\n",
        "output.index = row_names\n",
        "\n",
        "# CSV íŒŒì¼ë¡œ ì €ì¥ (UTF-8 ì¸ì½”ë”©)\n",
        "output.to_csv(\"output_soft_voting.csv\", index_label='fname', encoding='utf-8')\n",
        "\n",
        "print(\"output_soft_voting.csv íŒŒì¼ì´ ìƒì„±ë˜ì—ˆìŠµë‹ˆë‹¤.\")\n",
        "!pip install openai\n",
        "\n",
        "\n",
        "import pandas as pd\n",
        "import openai\n",
        "\n",
        "# OpenAI API í‚¤ ì„¤ì •\n",
        "openai.api_key = 'sk-uDS_ZTGi5aPxxxxxxxxxxxxxxxxxxxxxx'\n",
        "\n",
        "# CSV íŒŒì¼ ë¶ˆëŸ¬ì˜¤ê¸°\n",
        "output_df = pd.read_csv(\"/root/home/output_soft_voting.csv\")\n",
        "\n",
        "# ë²ˆì—­ í•¨ìˆ˜ ì •ì˜\n",
        "def translate_text(text, target_language='en'):\n",
        "    response = openai.Completion.create(\n",
        "        engine=\"text-davinci-003\",  # ì‚¬ìš©í•  ëª¨ë¸ ì´ë¦„\n",
        "        prompt=f\"Please translate the following text to {target_language}:\\n\\n{text}\",\n",
        "        max_tokens=500,\n",
        "        temperature=0.3\n",
        "    )\n",
        "    return response.choices[0].text.strip()\n",
        "\n",
        "# ë²ˆì—­í•  ìš”ì•½ë¬¸ ëª©ë¡\n",
        "summaries = output_df['summary'].tolist()\n",
        "\n",
        "# ë²ˆì—­ ê²°ê³¼ë¥¼ ì €ì¥í•  ë¦¬ìŠ¤íŠ¸\n",
        "translated_summaries = []\n",
        "\n",
        "# ê° ìš”ì•½ë¬¸ì„ ë²ˆì—­\n",
        "for summary in summaries:\n",
        "    translated_summary = translate_text(summary)\n",
        "    translated_summaries.append(translated_summary)\n",
        "\n",
        "# ë²ˆì—­ëœ ìš”ì•½ë¬¸ìœ¼ë¡œ ìƒˆë¡œìš´ DataFrame ìƒì„±\n",
        "translated_output = pd.DataFrame({'summary': translated_summaries})\n",
        "\n",
        "# í–‰ ì´ë¦„ ì„¤ì •\n",
        "num_rows = len(translated_output)\n",
        "row_names = [f'test_{i}' for i in range(num_rows)]\n",
        "\n",
        "# 466ë²ˆì§¸ í–‰ì€ ë¹„ìš°ê³ , 467ë²ˆì§¸ í–‰ë¶€í„° ì—°ì†ì ìœ¼ë¡œ ì´ë¦„ì„ ì„¤ì •\n",
        "for i in range(num_rows):\n",
        "    if i < 466:\n",
        "        row_names[i] = f'test_{i}'\n",
        "    elif i == 466:\n",
        "        row_names[i] = 'test_467'\n",
        "    elif i > 466:\n",
        "        row_names[i] = f'test_{i + 1}'\n",
        "\n",
        "translated_output.index = row_names\n",
        "\n",
        "# ë²ˆì—­ëœ ê²°ê³¼ë¥¼ CSV íŒŒì¼ë¡œ ì €ì¥ (UTF-8 ì¸ì½”ë”©)\n",
        "translated_output.to_csv(\"translated_output.csv\", index_label='fname', encoding='utf-8')\n",
        "\n",
        "print(\"translated_output.csv íŒŒì¼ì´ ìƒì„±ë˜ì—ˆìŠµë‹ˆë‹¤.\")\n",
        "\n",
        "!pip uninstall openai\n",
        "!pip install openai==0.28\n",
        "\n",
        "import pandas as pd\n",
        "import openai\n",
        "\n",
        "# OpenAI API í‚¤ ì„¤ì •\n",
        "\n",
        "# CSV íŒŒì¼ ë¶ˆëŸ¬ì˜¤ê¸°\n",
        "import pandas as pd\n",
        "import openai\n",
        "\n",
        "# OpenAI API í‚¤ ì„¤ì •\n",
        "openai.api_key = 'sk-uDS_ZTGi5aP0f-xxxxxxxxxxxxxx'\n",
        "\n",
        "# CSV íŒŒì¼ ë¶ˆëŸ¬ì˜¤ê¸°\n",
        "output_df = pd.read_csv(\"/root/home/output_soft_voting.csv\")\n",
        "import pandas as pd\n",
        "import openai\n",
        "\n",
        "# OpenAI API í‚¤ ì„¤ì •\n",
        "openai.api_key = 'sk-uDS_ZTGi5aP0f-xxxxxx'\n",
        "\n",
        "# CSV íŒŒì¼ ë¶ˆëŸ¬ì˜¤ê¸°\n",
        "output_df = pd.read_csv(\"/root/home/output_soft_voting.csv\")\n",
        "\n",
        "# ë¬¸ì¥ ê°œì„  í•¨ìˆ˜ ì •ì˜\n",
        "def improve_text(text):\n",
        "    response = openai.ChatCompletion.create(\n",
        "        model=\"gpt-3.5-turbo\",  # ì‚¬ìš©í•  ëª¨ë¸ ì´ë¦„\n",
        "        messages=[\n",
        "            {\"role\": \"system\", \"content\": \"You are a helpful assistant who improves the fluency of Korean text.\"},\n",
        "            {\"role\": \"user\", \"content\": f\"Please improve the fluency and naturalness of the following Korean text:\\n\\n{text}\"}\n",
        "        ],\n",
        "        max_tokens=500,\n",
        "        temperature=0.5\n",
        "    )\n",
        "    return response.choices[0].message['content'].strip()\n",
        "\n",
        "# ê°œì„ í•  ìš”ì•½ë¬¸ ëª©ë¡\n",
        "summaries = output_df['summary'].tolist()\n",
        "\n",
        "# ê°œì„ ëœ ë¬¸ì¥ ê²°ê³¼ë¥¼ ì €ì¥í•  ë¦¬ìŠ¤íŠ¸\n",
        "improved_summaries = []\n",
        "\n",
        "# ê° ë¬¸ì¥ì„ ê°œì„ \n",
        "for summary in summaries:\n",
        "    improved_summary = improve_text(summary)\n",
        "    improved_summaries.append(improved_summary)\n",
        "\n",
        "# ê°œì„ ëœ ë¬¸ì¥ìœ¼ë¡œ ìƒˆë¡œìš´ DataFrame ìƒì„±\n",
        "improved_output = pd.DataFrame({'summary': improved_summaries})\n",
        "\n",
        "# í–‰ ì´ë¦„ ì„¤ì •\n",
        "num_rows = len(improved_output)\n",
        "row_names = [f'test_{i}' for i in range(num_rows)]\n",
        "\n",
        "# 466ë²ˆì§¸ í–‰ì€ ë¹„ìš°ê³ , 467ë²ˆì§¸ í–‰ë¶€í„° ì—°ì†ì ìœ¼ë¡œ ì´ë¦„ì„ ì„¤ì •\n",
        "for i in range(num_rows):\n",
        "    if i < 466:\n",
        "        row_names[i] = f'test_{i}'\n",
        "    elif i == 466:\n",
        "        row_names[i] = 'test_467'\n",
        "    elif i > 466:\n",
        "        row_names[i] = f'test_{i + 1}'\n",
        "\n",
        "improved_output.index = row_names\n",
        "\n",
        "# ê°œì„ ëœ ê²°ê³¼ë¥¼ CSV íŒŒì¼ë¡œ ì €ì¥ (UTF-8 ì¸ì½”ë”©)\n",
        "improved_output.to_csv(\"improved_output.csv\", index_label='fname', encoding='utf-8')\n",
        "\n",
        "print(\"improved_output.csv íŒŒì¼ì´ ìƒì„±ë˜ì—ˆìŠµë‹ˆë‹¤.\")\n",
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from scipy import stats\n",
        "\n",
        "def translate_with_claude(text):\n",
        "    prompt = f\"Translate the following Korean text to English: {text}\"\n",
        "    try:\n",
        "        message = client.messages.create(\n",
        "            model=\"claude-3-opus-20240229\",\n",
        "            max_tokens=300,\n",
        "            temperature=0.7,\n",
        "            messages=[\n",
        "                {\"role\": \"user\", \"content\": prompt}\n",
        "            ]\n",
        "        )\n",
        "        return message.content[0].text.strip()\n",
        "    except Exception as e:\n",
        "        print(f\"Error in translation: {e}\")\n",
        "        return text  # Return original text if translation fails\n",
        "\n",
        "# Anthropic API í‚¤ ì„¤ì • (ì´ ë¶€ë¶„ì„ ì½”ë“œ ì‹œì‘ ë¶€ë¶„ìœ¼ë¡œ ì˜®ê¹ë‹ˆë‹¤)\n",
        "os.environ[\"ANTHROPIC_API_KEY\"] = \"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\"\n",
        "client = anthropic.Anthropic()\n",
        "\n",
        "# ê²°ê³¼ë¥¼ DataFrameìœ¼ë¡œ ì €ì¥í•˜ê³  ë²ˆì—­ ì ìš©\n",
        "output = pd.DataFrame({'summary': final_predictions})\n",
        "output['translated_summary'] = [translate_with_claude(text) for text in tqdm(output['summary'], desc=\"Translating\")]\n",
        "\n",
        "\n",
        "# íŒŒì¼ ì´ë¦„ ë¦¬ìŠ¤íŠ¸\n",
        "file_names = [f\"output ({i}).csv\" for i in range(16, 26)]\n",
        "\n",
        "# ì˜ˆì¸¡ ê²°ê³¼ë¥¼ ì €ì¥í•  ë¦¬ìŠ¤íŠ¸\n",
        "predictions = []\n",
        "\n",
        "# ê° íŒŒì¼ì„ ë¶ˆëŸ¬ì™€ì„œ ì˜ˆì¸¡ ê²°ê³¼ë¥¼ ë¦¬ìŠ¤íŠ¸ì— ì €ì¥\n",
        "for file_name in file_names:\n",
        "    df = pd.read_csv(file_name)\n",
        "    predictions.append(df['summary'])\n",
        "\n",
        "# ë°ì´í„°í”„ë ˆì„ìœ¼ë¡œ ë³€í™˜\n",
        "pred_df = pd.DataFrame(predictions).T\n",
        "\n",
        "# ëª¨ë¸ ê°€ì¤‘ì¹˜ ì„¤ì • (ì˜ˆ: ì„±ëŠ¥ì´ ì¢‹ì€ ëª¨ë¸ì— ë” ë†’ì€ ê°€ì¤‘ì¹˜ ë¶€ì—¬)\n",
        "model_weights = np.random.dirichlet(np.ones(len(file_names)), size=1)[0]\n",
        "\n",
        "def improved_voting(row):\n",
        "    # ê° ì˜ˆì¸¡ì˜ ë¹ˆë„ ê³„ì‚°\n",
        "    value_counts = row.value_counts()\n",
        "\n",
        "    # ê°€ì¤‘ì¹˜ë¥¼ ì ìš©í•œ íˆ¬í‘œ\n",
        "    weighted_votes = {}\n",
        "    for i, pred in enumerate(row):\n",
        "        if pred in weighted_votes:\n",
        "            weighted_votes[pred] += model_weights[i]\n",
        "        else:\n",
        "            weighted_votes[pred] = model_weights[i]\n",
        "\n",
        "    # ìµœëŒ€ íˆ¬í‘œ ìˆ˜ í™•ì¸\n",
        "    max_votes = max(weighted_votes.values())\n",
        "\n",
        "    # ìµœëŒ€ íˆ¬í‘œë¥¼ ë°›ì€ ì˜ˆì¸¡ë“¤\n",
        "    top_predictions = [k for k, v in weighted_votes.items() if v == max_votes]\n",
        "\n",
        "    # ìµœë¹ˆê°’ì´ ì—¬ëŸ¬ ê°œì¼ ê²½ìš° ëœë¤ ì„ íƒ\n",
        "    if len(top_predictions) > 1:\n",
        "        return np.random.choice(top_predictions)\n",
        "    else:\n",
        "        return top_predictions[0]\n",
        "\n",
        "# ê°œì„ ëœ íˆ¬í‘œ ë°©ì‹ ì ìš©\n",
        "final_predictions = pred_df.apply(improved_voting, axis=1)\n",
        "\n",
        "# ê²°ê³¼ë¥¼ DataFrameìœ¼ë¡œ ì €ì¥\n",
        "output = pd.DataFrame({'summary': final_predictions})\n",
        "\n",
        "# í–‰ ì´ë¦„ ì§€ì •\n",
        "num_rows = len(output)\n",
        "row_names = [f'test_{i}' for i in range(num_rows)]\n",
        "\n",
        "# 466ë²ˆì§¸ í–‰ì€ ë¹„ìš°ê³ , 467ë²ˆì§¸ í–‰ë¶€í„° ì—°ì†ì ìœ¼ë¡œ ì´ë¦„ì„ ì„¤ì •\n",
        "for i in range(num_rows):\n",
        "    if i < 466:\n",
        "        row_names[i] = f'test_{i}'\n",
        "    elif i == 466:\n",
        "        row_names[i] = 'test_467'\n",
        "    elif i > 466:\n",
        "        row_names[i] = f'test_{i + 1}'\n",
        "\n",
        "output.index = row_names\n",
        "\n",
        "# CSV íŒŒì¼ë¡œ ì €ì¥ (UTF-8 ì¸ì½”ë”©)\n",
        "output.to_csv(\"output_improved_voting.csv\", index_label='fname', encoding='utf-8')\n",
        "\n",
        "print(\"output_improved_voting.csv íŒŒì¼ì´ ìƒì„±ë˜ì—ˆìŠµë‹ˆë‹¤.\")\n",
        "!pip install numpy\n",
        "!pip install anthropic\n",
        "!pip install --upgrade anthropic\n",
        "\n",
        "\n",
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from scipy import stats\n",
        "import anthropic\n",
        "import os\n",
        "from tqdm import tqdm\n",
        "\n",
        "# Import tqdm if not already imported\n",
        "from tqdm import tqdm\n",
        "\n",
        "# ... (rest of your code)\n",
        "\n",
        "# ê²°ê³¼ë¥¼ DataFrameìœ¼ë¡œ ì €ì¥í•˜ê³  ë²ˆì—­ ì ìš©\n",
        "output = pd.DataFrame({'summary': final_predictions})\n",
        "output['translated_summary'] = [translate_with_claude(text) for text in tqdm(output['summary'], desc=\"Translating\")]\n",
        "\n",
        "\n",
        "# Anthropic API í‚¤ ì„¤ì •\n",
        "os.environ[\"ANTHROPIC_API_KEY\"] = \"sk-ant-api03-xxxxxxxxxxxxxxxxxxxx8kOGc_O7SbAgnSAFpDhJaMw-F9goewAA\"\n",
        "client = anthropic.Client()\n",
        "\n",
        "# íŒŒì¼ ì´ë¦„ ë¦¬ìŠ¤íŠ¸\n",
        "file_names = [f\"output ({i}).csv\" for i in range(16, 26)]\n",
        "\n",
        "# ì˜ˆì¸¡ ê²°ê³¼ë¥¼ ì €ì¥í•  ë¦¬ìŠ¤íŠ¸\n",
        "predictions = []\n",
        "\n",
        "# ê° íŒŒì¼ì„ ë¶ˆëŸ¬ì™€ì„œ ì˜ˆì¸¡ ê²°ê³¼ë¥¼ ë¦¬ìŠ¤íŠ¸ì— ì €ì¥\n",
        "for file_name in file_names:\n",
        "    df = pd.read_csv(file_name)\n",
        "    predictions.append(df['summary'])\n",
        "\n",
        "# ë°ì´í„°í”„ë ˆì„ìœ¼ë¡œ ë³€í™˜\n",
        "pred_df = pd.DataFrame(predictions).T\n",
        "\n",
        "# ëª¨ë¸ ê°€ì¤‘ì¹˜ ì„¤ì • (ì˜ˆ: ì„±ëŠ¥ì´ ì¢‹ì€ ëª¨ë¸ì— ë” ë†’ì€ ê°€ì¤‘ì¹˜ ë¶€ì—¬)\n",
        "model_weights = np.random.dirichlet(np.ones(len(file_names)), size=1)[0]\n",
        "\n",
        "def improved_voting(row):\n",
        "    # ê° ì˜ˆì¸¡ì˜ ë¹ˆë„ ê³„ì‚°\n",
        "    value_counts = row.value_counts()\n",
        "\n",
        "    # ê°€ì¤‘ì¹˜ë¥¼ ì ìš©í•œ íˆ¬í‘œ\n",
        "    weighted_votes = {}\n",
        "    for i, pred in enumerate(row):\n",
        "        if pred in weighted_votes:\n",
        "            weighted_votes[pred] += model_weights[i]\n",
        "        else:\n",
        "            weighted_votes[pred] = model_weights[i]\n",
        "\n",
        "    # ìµœëŒ€ íˆ¬í‘œ ìˆ˜ í™•ì¸\n",
        "    max_votes = max(weighted_votes.values())\n",
        "\n",
        "    # ìµœëŒ€ íˆ¬í‘œë¥¼ ë°›ì€ ì˜ˆì¸¡ë“¤\n",
        "    top_predictions = [k for k, v in weighted_votes.items() if v == max_votes]\n",
        "\n",
        "    # ìµœë¹ˆê°’ì´ ì—¬ëŸ¬ ê°œì¼ ê²½ìš° ëœë¤ ì„ íƒ\n",
        "    if len(top_predictions) > 1:\n",
        "        return np.random.choice(top_predictions)\n",
        "    else:\n",
        "        return top_predictions[0]\n",
        "\n",
        "# ê°œì„ ëœ íˆ¬í‘œ ë°©ì‹ ì ìš©\n",
        "final_predictions = pred_df.apply(improved_voting, axis=1)\n",
        "\n",
        "# Claude APIë¥¼ ì‚¬ìš©í•œ ë²ˆì—­ í•¨ìˆ˜\n",
        "def translate_with_claude(text):\n",
        "    prompt = f\"Translate the following Korean text to English: {text}\"\n",
        "    response = client.completion(\n",
        "        model=\"claude-2\",\n",
        "        prompt=prompt,\n",
        "        max_tokens_to_sample=300,\n",
        "        temperature=0.7,\n",
        "    )\n",
        "    return response.completion.strip()\n",
        "\n",
        "# ê²°ê³¼ë¥¼ DataFrameìœ¼ë¡œ ì €ì¥í•˜ê³  ë²ˆì—­ ì ìš©\n",
        "output = pd.DataFrame({'summary': final_predictions})\n",
        "output['translated_summary'] = output['summary'].progress_apply(translate_with_claude)\n",
        "\n",
        "# í–‰ ì´ë¦„ ì§€ì •\n",
        "num_rows = len(output)\n",
        "row_names = [f'test_{i}' for i in range(num_rows)]\n",
        "\n",
        "# 466ë²ˆì§¸ í–‰ì€ ë¹„ìš°ê³ , 467ë²ˆì§¸ í–‰ë¶€í„° ì—°ì†ì ìœ¼ë¡œ ì´ë¦„ì„ ì„¤ì •\n",
        "for i in range(num_rows):\n",
        "    if i < 466:\n",
        "        row_names[i] = f'test_{i}'\n",
        "    elif i == 466:\n",
        "        row_names[i] = 'test_467'\n",
        "    elif i > 466:\n",
        "        row_names[i] = f'test_{i + 1}'\n",
        "\n",
        "output.index = row_names\n",
        "\n",
        "# CSV íŒŒì¼ë¡œ ì €ì¥ (UTF-8 ì¸ì½”ë”©)\n",
        "output.to_csv(\"output_improved_voting_with_translation.csv\", index_label='fname', encoding='utf-8')\n",
        "\n",
        "print(\"output_improved_voting_with_translation.csv íŒŒì¼ì´ ìƒì„±ë˜ì—ˆìŠµë‹ˆë‹¤.\")\n"
      ]
    }
  ]
}
